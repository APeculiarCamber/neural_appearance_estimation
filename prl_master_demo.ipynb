{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL APPEARNACE ESTIMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "def z_scoring(x_rep, thresh=1.5):\n",
    "    pr_rep = torch.norm(x_rep, dim=1)\n",
    "    pr_min, pr_max = pr_rep.min(), pr_rep.max()\n",
    "    pr_rep = (pr_rep - pr_min) / (pr_max - pr_min)\n",
    "    mean = pr_rep.mean((1, 2))  \n",
    "    std = torch.std(pr_rep)\n",
    "    z_scores = (pr_rep - mean[:,None,None]) / std\n",
    "    outliers = z_scores > thresh\n",
    "    return outliers\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "def display_linear_image(tensor):\n",
    "    disp_tensor = tensor# ** (1.0 / 2.2)\n",
    "    disp_tensor = torch.nan_to_num(torch.clamp(disp_tensor, 0.0, 1.0), 0.0)\n",
    "    image = to_pil(disp_tensor.squeeze())\n",
    "    display(image)\n",
    "\n",
    "def display_neural_material(neural_mat):\n",
    "    neural_mat_norm = torch.norm(neural_mat.squeeze(), p=2, dim=0, keepdim=True).repeat(3, 1, 1)\n",
    "    filted_features = neural_mat_norm * torch.logical_not(z_scoring(neural_mat, thresh=2.0))[None]\n",
    "    pr_min, pr_max = filted_features.min(), filted_features.max()\n",
    "    pr_rep = torch.clamp((neural_mat_norm - pr_min) / (pr_max - pr_min), 0.0, 1.0)\n",
    "    \n",
    "    image = to_pil(pr_rep)\n",
    "    display(image)\n",
    "\n",
    "def display_video(video_tensor):\n",
    "    video_tensor = torch.clamp(torch.nan_to_num(video_tensor, 0.0), 0.0, 1.0)\n",
    "    video_np = video_tensor.cpu().numpy()\n",
    "    \n",
    "    frames = []\n",
    "    for frame in video_np:\n",
    "        frame_display = np.transpose(frame, (1, 2, 0))\n",
    "        frames.append(frame_display)\n",
    "\n",
    "    with BytesIO() as gif:\n",
    "        frames[0] = Image.fromarray((frames[0] * 255).astype(np.uint8))\n",
    "        frames[0].save(gif, format='gif', save_all=True, append_images=[Image.fromarray((frame * 255).astype(np.uint8)) for frame in frames[1:]], loop=0)\n",
    "        gif.seek(0)\n",
    "        display(IPImage(data=gif.getvalue(), format='gif'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    m_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    m_device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", m_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from brdf_utils import loadImage, load_exr\n",
    "# TODO: pick a few good ones\n",
    "input_photo_dir = \"demo_images\"\n",
    "input_photo_names = [fn for fn in os.listdir(input_photo_dir)]# if '.png' in fn.lower()]\n",
    "print(input_photo_names)\n",
    "\n",
    "photo_id = 5\n",
    "\n",
    "input_photo = loadImage(os.path.join(input_photo_dir, input_photo_names[photo_id]))\n",
    "input_photo = input_photo.unsqueeze(0).to(m_device)\n",
    "input_photo = input_photo #** 2.2 # already in linear color-space\n",
    "in_height, in_width = input_photo.shape[-2:]\n",
    "display_linear_image(input_photo.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    return num_params\n",
    "    \n",
    "from models.prl_net import RelitPixelNet\n",
    "from models.prl_feature_injector import SineDirectionEncoder_NN\n",
    "from models.prl_resnet import GenResNetReplicateHA\n",
    "from models.prl_pixel_mlp import *\n",
    "model_path = \"model_data/ldr_final_prl_model.pth\"\n",
    "model : RelitPixelNet = torch.load(model_path)\n",
    "model = model.eval().to(m_device)\n",
    "\n",
    "m_est : GenResNetReplicateHA = model.resnet\n",
    "m_render : NaiveMLPRenderer = model.render_net\n",
    "out_direction_encoder : SineDirectionEncoder_NN = model.render_encoder\n",
    "print(type(model.space_manager.compress_input))\n",
    "\n",
    "print(f\"M_est has {get_num_parameters(m_est)} parameters.\")\n",
    "print(f\"M_render has {get_num_parameters(m_render)} parameters.\")\n",
    "print(f\"The positional encoder, ND_enc, {get_num_parameters(out_direction_encoder)} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE NEURAL MATERIAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brdf_render import generateDirectionMaps\n",
    "with torch.no_grad():\n",
    "    model = model.eval().to(m_device)\n",
    "    view_position = torch.tensor([[0.0, 0.0, 4.0]], device=m_device) # 4.0 â‰ˆ 1.0 / tan((28 / 2) degrees); for an FOV of 28 degrees\n",
    "    light_position = view_position # colocated light and view for input\n",
    "\n",
    "    # Convert light and view positions to normalized direction vectors for each pixel of the input\n",
    "    light_dirs, view_dirs = generateDirectionMaps(light_position, view_position, in_height)\n",
    "\n",
    "    # Make the neural material parameters, function handles log-relative mapping\n",
    "    neural_mat = model.make_neural_feature_rep(input_photo.to(m_device), light_dirs, view_dirs)\n",
    "    display_neural_material(neural_mat.clone().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE RE-RENDERS WITH THE NEURAL MATERIAL MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = model.eval().to(m_device)\n",
    "    # MAKE LIGHT POSITIONS IN A CIRCLE ABOVE THE MATERIAL\n",
    "    k = 32 # number of samples\n",
    "    radius = 1.0 # radius of light\n",
    "    light_height = 3.0\n",
    "    light_height = torch.tensor([[light_height]] * k, device=m_device)\n",
    "    angles = ((torch.arange(0, k, dtype=torch.float, device=m_device) / k) * torch.pi * 2)[:,None]\n",
    "    light_position = torch.cat([torch.sin(angles) * radius, torch.cos(angles) * radius, light_height], dim=1)\n",
    "    view_position = torch.tensor([[0.0, 0.0, 4.0]] * k, device=m_device)\n",
    "\n",
    "    # MAKE DIRECTIONS\n",
    "    lt, vt = generateDirectionMaps(light_position, view_position, in_height)\n",
    "\n",
    "    # RENDER\n",
    "    log_rerender = model.render_from_neural_rep_multi(neural_mat, lt, vt).squeeze()\n",
    "    circle_renders = model.space_manager.decompress_target_call(log_rerender)\n",
    "\n",
    "    # DISPLAY\n",
    "    print(\"Rendering Done. Displaying:\")\n",
    "    display_video((circle_renders.squeeze()).cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = model.eval().to(m_device)\n",
    "    lv = torch.tensor([[0.0, 0.0, 4.0]], device=m_device)\n",
    "    lt, vt = generateDirectionMaps(lv, lv, 256)\n",
    "    ident_render, _ = model.render_multi(input_photo, lt, vt, lt, vt)\n",
    "    display_linear_image(ident_render)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
